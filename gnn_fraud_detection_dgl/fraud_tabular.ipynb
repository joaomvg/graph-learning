{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile        fd_sl_train_entry_point.py  \u001b[0m\u001b[01;34mmodel-dir\u001b[0m/          \u001b[01;35mroc_curve.png\u001b[0m\n",
      "__init__.py       fraud_tabular.ipynb         \u001b[01;34moutput-dir\u001b[0m/         utils.py\n",
      "\u001b[01;34m__pycache__\u001b[0m/      graph_data_preprocessor.py  \u001b[01;35mpr_curve.png\u001b[0m\n",
      "data.py           graph_utils.py              \u001b[01;34mpreprocessed_data\u001b[0m/\n",
      "estimator_fns.py  \u001b[01;34minput_data\u001b[0m/                 pytorch_model.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLG version: 0.7.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "import torch as th\n",
    "import dgl\n",
    "print('DLG version: {}'.format(dgl.__version__))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from estimator_fns import parse_args, get_logger\n",
    "from graph_utils import get_files, construct_graph\n",
    "from data import get_features, get_labels, read_masked_nodes, parse_edgelist, read_edges\n",
    "from utils import get_metrics\n",
    "from pytorch_model import HeteroRGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('https://hr-projects-assets-prod.s3.amazonaws.com/g2m06b3p570/e25b2de8258d6972dd6aa7e5106aa8a9/fraud_detection_challenge_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tid', 'sid', 'mid', 'payment_type', 'label', 'date', 'f0', 'f1', 'f2',\n",
       "       'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13',\n",
       "       'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23',\n",
       "       'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DT'] = (data['date'] - data['date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACTION_ID_COL = 'tid'\n",
    "LABEL_COL = 'label'\n",
    "DATE_COL = 'date'\n",
    "ID_COLS = ['sid','mid']\n",
    "CATEGORICAL_COLS = ['payment_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data-dir', type=str, default='input_data')\n",
    "    parser.add_argument('--output-dir', type=str, default='preprocessed_data')\n",
    "    parser.add_argument('--transactions', type=str, default='transaction.csv', help='name of file with transactions')\n",
    "    parser.add_argument('--identity', type=str, default='identity.csv', help='name of file with identity info')\n",
    "    parser.add_argument('--id-cols', type=str, default= args_ids, help='comma separated id cols in transactions table')\n",
    "    parser.add_argument('--cat-cols', type=str, default= cat_cols, help='comma separated categorical cols in transactions')\n",
    "    parser.add_argument('--train-data-ratio', type=float, default=0.8, help='fraction of data to use in training set')\n",
    "    parser.add_argument('--construct-homogeneous', action=\"store_true\", default=False,\n",
    "                        help='use bipartite graphs edgelists to construct homogenous graph edgelist')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def get_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    log_format = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n",
    "    logging.basicConfig(format=log_format, level=logging.INFO)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def load_data(transaction_data: pd.DataFrame, train_data_ratio: float):\n",
    "    transaction_df = transaction_data\n",
    "    logging.info(\"Shape of transaction data is {}\".format(transaction_df.shape))\n",
    "    logging.info(\"# Tagged transactions: {}\".format(len(transaction_df) - transaction_df[LABEL_COL].isnull().sum()))\n",
    "\n",
    "\n",
    "    # extract out transactions for test/validation\n",
    "    n_train = int(transaction_df.shape[0]*train_data_ratio)\n",
    "    test_ids = transaction_df[TRANSACTION_ID_COL].values[n_train:]\n",
    "\n",
    "    get_fraud_frac = lambda series: 100 * sum(series)/len(series)\n",
    "    logging.info(\"Percent fraud for train transactions: {}\".format(get_fraud_frac(transaction_df[LABEL_COL][:n_train])))\n",
    "    logging.info(\"Percent fraud for test transactions: {}\".format(get_fraud_frac(transaction_df[LABEL_COL][n_train:])))\n",
    "    logging.info(\"Percent fraud for all transactions: {}\".format(get_fraud_frac(transaction_df[LABEL_COL])))\n",
    "\n",
    "\n",
    "    return transaction_df, test_ids\n",
    "\n",
    "\n",
    "def get_features_and_labels(transactions_df: pd.DataFrame, transactions_id_cols: list, transactions_cat_cols: list):\n",
    "    # Get features\n",
    "    non_feature_cols = [LABEL_COL, DATE_COL] + transactions_id_cols\n",
    "    feature_cols = [col for col in transactions_df.columns if col not in non_feature_cols]\n",
    "    logging.info(\"Categorical columns: {}\".format(transactions_cat_cols))\n",
    "    features = pd.get_dummies(transactions_df[feature_cols], columns=transactions_cat_cols).fillna(0)\n",
    "    #features['TransactionAmt'] = features['TransactionAmt'].apply(np.log10)\n",
    "    logging.info(\"Transformed feature columns: {}\".format(list(features.columns)))\n",
    "    logging.info(\"Shape of features: {}\".format(features.shape))\n",
    "    processed_data['features'] = features\n",
    "\n",
    "    # Get labels\n",
    "    processed_data['labels'] = transactions_df[[TRANSACTION_ID_COL, LABEL_COL]].copy()\n",
    "\n",
    "def get_relations_and_edgelist(transactions_df: pd.DataFrame, transactions_id_cols: list):\n",
    "    # Get relations\n",
    "    edge_types = transactions_id_cols\n",
    "    logging.info(\"Found the following distinct relation types: {}\".format(edge_types))\n",
    "    id_cols = [TRANSACTION_ID_COL] + transactions_id_cols\n",
    "    full_identity_df = transactions_df[id_cols]\n",
    "    logging.info(\"Shape of identity columns: {}\".format(full_identity_df.shape))\n",
    "\n",
    "    # extract edges\n",
    "    edges = {}\n",
    "    for etype in edge_types:\n",
    "        edgelist = full_identity_df[[TRANSACTION_ID_COL, etype]].dropna()\n",
    "        processed_data['relation_{}_edgelist'.format(etype)] = edgelist\n",
    "        edges[etype] = edgelist\n",
    "    return edges\n",
    "\n",
    "\n",
    "def create_homogeneous_edgelist(edges):\n",
    "    homogeneous_edges = []\n",
    "    for etype, relations in edges.items():\n",
    "        for edge_relation, frame in relations.groupby(etype):\n",
    "            new_edges = [(a, b) for (a, b) in combinations(frame[TRANSACTION_ID_COL].values, 2)\n",
    "                         if (a, b) not in homogeneous_edges and (b, a) not in homogeneous_edges]\n",
    "            homogeneous_edges.extend(new_edges)\n",
    "\n",
    "    processed_data['homogeneous_edges'] = homogeneous_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 13:23:32,315 INFO __main__: Shape of transaction data is (45954, 38)\n",
      "2022-10-30 13:23:32,317 INFO __main__: # Tagged transactions: 45954\n",
      "2022-10-30 13:23:32,321 INFO __main__: Percent fraud for train transactions: 2.3365884177025813\n",
      "2022-10-30 13:23:32,323 INFO __main__: Percent fraud for test transactions: 63.3010553802633\n",
      "2022-10-30 13:23:32,326 INFO __main__: Percent fraud for all transactions: 14.52974713844279\n",
      "2022-10-30 13:23:32,327 INFO __main__: Categorical columns: ['payment_type']\n",
      "2022-10-30 13:23:32,351 INFO __main__: Transformed feature columns: ['tid', 'f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'payment_type_a', 'payment_type_b', 'payment_type_c', 'payment_type_d', 'payment_type_e']\n",
      "2022-10-30 13:23:32,352 INFO __main__: Shape of features: (45954, 38)\n",
      "2022-10-30 13:23:32,356 INFO __main__: Found the following distinct relation types: ['sid', 'mid']\n",
      "2022-10-30 13:23:32,359 INFO __main__: Shape of identity columns: (45954, 3)\n"
     ]
    }
   ],
   "source": [
    "construct_homogeneous = False\n",
    "\n",
    "transactions = data.drop(columns= ['DT']).copy()\n",
    "transactions, test_transactions = load_data(transactions,\n",
    "                                            0.8)\n",
    "get_features_and_labels(transactions, ID_COLS, CATEGORICAL_COLS)\n",
    "relational_edges = get_relations_and_edgelist(transactions, ID_COLS)\n",
    "\n",
    "if construct_homogeneous:\n",
    "    create_homogeneous_edgelist(relational_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>payment_type_a</th>\n",
       "      <th>payment_type_b</th>\n",
       "      <th>payment_type_c</th>\n",
       "      <th>payment_type_d</th>\n",
       "      <th>payment_type_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.022376</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.823592</td>\n",
       "      <td>0.497025</td>\n",
       "      <td>0.965457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.941257</td>\n",
       "      <td>0.217850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.057230</td>\n",
       "      <td>0.151243</td>\n",
       "      <td>0.841056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.930203</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.159374</td>\n",
       "      <td>0.697188</td>\n",
       "      <td>0.195489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45949</th>\n",
       "      <td>45949</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.695066</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>0.119490</td>\n",
       "      <td>0.348379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457711</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.368159</td>\n",
       "      <td>0.303483</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45950</th>\n",
       "      <td>45950</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.573930</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.500364</td>\n",
       "      <td>0.223533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.482587</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45951</th>\n",
       "      <td>45951</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.350026</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.621975</td>\n",
       "      <td>0.122220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.482587</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45952</th>\n",
       "      <td>45952</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.096758</td>\n",
       "      <td>0.298064</td>\n",
       "      <td>0.530217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.442786</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.587065</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45953</th>\n",
       "      <td>45953</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.350026</td>\n",
       "      <td>0.428682</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.398457</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.680125</td>\n",
       "      <td>0.169226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.442786</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.587065</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45954 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tid        f0        f1        f2        f3        f4        f5  \\\n",
       "0          0  0.022376  0.070495  0.428682  0.999985  0.999985  0.398457   \n",
       "1          1  0.024928  0.999985  0.999985  0.999985  0.999985  0.398457   \n",
       "2          2  0.006173  0.070495  0.428682  0.999985  0.999985  0.398457   \n",
       "3          3  0.017375  0.070495  0.428682  0.999985  0.999985  0.398457   \n",
       "4          4  0.009081  0.999985  0.999985  0.999985  0.999985  0.398457   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "45949  45949  0.009081  0.695066  0.428682  0.999985  0.999985  0.999985   \n",
       "45950  45950  0.003175  0.573930  0.999985  0.999985  0.022791  0.398457   \n",
       "45951  45951  0.009081  0.350026  0.428682  0.999985  0.999985  0.398457   \n",
       "45952  45952  0.006173  0.070495  0.999985  0.999985  0.999985  0.999985   \n",
       "45953  45953  0.003175  0.350026  0.428682  0.999985  0.022791  0.398457   \n",
       "\n",
       "             f6        f7        f8  ...       f27       f28       f29  \\\n",
       "0      0.823592  0.497025  0.965457  ...  0.009950  0.014925  0.592040   \n",
       "1      0.999985  0.941257  0.217850  ...  0.009950  0.014925  0.592040   \n",
       "2      0.057230  0.151243  0.841056  ...  0.009950  0.014925  0.592040   \n",
       "3      0.999985  0.930203  0.025402  ...  0.009950  0.014925  0.592040   \n",
       "4      0.159374  0.697188  0.195489  ...  0.009950  0.014925  0.592040   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "45949  0.014230  0.119490  0.348379  ...  0.457711  0.268657  0.368159   \n",
       "45950  0.999985  0.500364  0.223533  ...  0.601990  0.402985  0.482587   \n",
       "45951  0.999985  0.621975  0.122220  ...  0.601990  0.402985  0.482587   \n",
       "45952  0.096758  0.298064  0.530217  ...  0.781095  0.855721  0.442786   \n",
       "45953  0.999985  0.680125  0.169226  ...  0.781095  0.855721  0.442786   \n",
       "\n",
       "            f30       f31  payment_type_a  payment_type_b  payment_type_c  \\\n",
       "0      0.139303  0.497512               0               0               0   \n",
       "1      0.139303  0.497512               0               0               1   \n",
       "2      0.139303  0.497512               0               0               0   \n",
       "3      0.139303  0.497512               1               0               0   \n",
       "4      0.139303  0.497512               0               0               1   \n",
       "...         ...       ...             ...             ...             ...   \n",
       "45949  0.303483  0.845771               0               0               0   \n",
       "45950  0.800995  0.164179               0               0               0   \n",
       "45951  0.800995  0.164179               0               0               0   \n",
       "45952  0.447761  0.587065               0               0               0   \n",
       "45953  0.447761  0.587065               0               0               0   \n",
       "\n",
       "       payment_type_d  payment_type_e  \n",
       "0                   0               1  \n",
       "1                   0               0  \n",
       "2                   0               1  \n",
       "3                   0               0  \n",
       "4                   0               0  \n",
       "...               ...             ...  \n",
       "45949               0               1  \n",
       "45950               0               1  \n",
       "45951               1               0  \n",
       "45952               0               1  \n",
       "45953               0               1  \n",
       "\n",
       "[45954 rows x 38 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature_matrix):\n",
    "    mean = th.mean(feature_matrix, axis=0)\n",
    "    stdev = th.sqrt(th.sum((feature_matrix - mean)**2, axis=0)/feature_matrix.shape[0])\n",
    "    return mean, stdev, (feature_matrix - mean) / stdev\n",
    "\n",
    "\n",
    "def train_fg(model, optim, loss, features, labels, train_g, test_g, test_mask,\n",
    "             device, n_epochs, thresh, compute_metrics=True):\n",
    "    \"\"\"\n",
    "    A full graph verison of RGCN training\n",
    "    \"\"\"\n",
    "    train_mask = th.logical_not(test_mask)\n",
    "    train_idx = th.nonzero(train_mask, as_tuple=True)[0]\n",
    "    test_idx = th.nonzero(test_mask, as_tuple=True)[0]\n",
    "\n",
    "    print(\"Train label: {}\".format(train_mask.sum()))\n",
    "    print(\"Test label: {}\".format(test_mask.sum()))\n",
    "\n",
    "    duration = []\n",
    "    for epoch in range(n_epochs):\n",
    "        tic = time.time()\n",
    "        loss_val = 0.\n",
    "\n",
    "        pred = model(train_g, features.to(device))\n",
    "\n",
    "        l = loss(th.index_select(pred, 0, train_idx),\n",
    "                 th.index_select(labels, 0, train_idx))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_val += l\n",
    "\n",
    "        duration.append(time.time() - tic)\n",
    "        metric = evaluate(model, train_g, features, labels, device)\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | f1 {:.4f} \".format(\n",
    "                epoch, np.mean(duration), loss_val, metric))\n",
    "\n",
    "    class_preds, pred_proba = get_model_class_predictions(model,\n",
    "                                                          test_g,\n",
    "                                                          features,\n",
    "                                                          labels,\n",
    "                                                          device,\n",
    "                                                          threshold=thresh)\n",
    "\n",
    "    if compute_metrics:\n",
    "        acc, f1, p, r, roc, pr, ap, cm = get_metrics(class_preds, pred_proba, labels.numpy(), test_mask.numpy(), './')\n",
    "        print(\"Metrics\")\n",
    "        print(\"\"\"Confusion Matrix:\n",
    "                                {}\n",
    "                                f1: {:.4f}, precision: {:.4f}, recall: {:.4f}, acc: {:.4f}, roc: {:.4f}, pr: {:.4f}, ap: {:.4f}\n",
    "                             \"\"\".format(cm, f1, p, r, acc, roc, pr, ap))\n",
    "\n",
    "    return model, class_preds, pred_proba\n",
    "\n",
    "\n",
    "def get_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Only works for binary case.\n",
    "    Attention!\n",
    "    tn, fp, fn, tp = cf_m[0,0],cf_m[0,1],cf_m[1,0],cf_m[1,1]\n",
    "\n",
    "    :param y_true: A list of labels in 0 or 1: 1 * N\n",
    "    :param y_pred: A list of labels in 0 or 1: 1 * N\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # print(y_true, y_pred)\n",
    "\n",
    "    cf_m = confusion_matrix(y_true, y_pred)\n",
    "    # print(cf_m)\n",
    "\n",
    "    precision = cf_m[1,1] / (cf_m[1,1] + cf_m[0,1] + 10e-5)\n",
    "    recall = cf_m[1,1] / (cf_m[1,1] + cf_m[1,0])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 10e-5)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(model, g, features, labels, device):\n",
    "    \"Compute the F1 value in a binary classification case\"\n",
    "\n",
    "    preds = model(g, features.to(device))\n",
    "    preds = th.argmax(preds, axis=1).numpy()\n",
    "    precision, recall, f1 = get_f1_score(labels, preds)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def get_model_class_predictions(model, g, features, labels, device, threshold=None):\n",
    "    unnormalized_preds = model(g, features.to(device))\n",
    "    pred_proba = th.softmax(unnormalized_preds, dim=-1)\n",
    "    if not threshold:\n",
    "        return unnormalized_preds.argmax(axis=1).detach().numpy(), pred_proba[:,1].detach().numpy()\n",
    "    return np.where(pred_proba.detach().numpy() > threshold, 1, 0), pred_proba[:,1].detach().numpy()\n",
    "\n",
    "\n",
    "def save_model(g, model, model_dir, id_to_node, mean, stdev):\n",
    "\n",
    "    # Save Pytorch model's parameters to model.pth\n",
    "    th.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n",
    "\n",
    "    # Save graph's structure information to metadata.pkl for inference codes to initialize RGCN model.\n",
    "    etype_list = g.canonical_etypes\n",
    "    ntype_cnt = {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes}\n",
    "    with open(os.path.join(model_dir, 'metadata.pkl'), 'wb') as f:\n",
    "        pickle.dump({'etypes': etype_list,\n",
    "                     'ntype_cnt': ntype_cnt,\n",
    "                     'feat_mean': mean,\n",
    "                     'feat_std': stdev}, f)\n",
    "\n",
    "    # Save original IDs to Node_ids, and trained embedding for non-target node type\n",
    "    # Covert id_to_node into pandas dataframes\n",
    "    for ntype, mapping in id_to_node.items():\n",
    "\n",
    "        # ignore target node\n",
    "        if ntype == 'target':\n",
    "            continue\n",
    "\n",
    "        # retrieve old and node id list\n",
    "        old_id_list, node_id_list = [], []\n",
    "        for old_id, node_id in mapping.items():\n",
    "            old_id_list.append(old_id)\n",
    "            node_id_list.append(node_id)\n",
    "\n",
    "        # retrieve embeddings of a node type\n",
    "        node_feats = model.embed[ntype].detach().numpy()\n",
    "\n",
    "        # get the number of nodes and the dimension of features\n",
    "        num_nodes = node_feats.shape[0]\n",
    "        num_feats = node_feats.shape[1]\n",
    "\n",
    "        # create id dataframe\n",
    "        node_ids_df = pd.DataFrame({'~label': [ntype] * num_nodes})\n",
    "        node_ids_df['~id_tmp'] = old_id_list\n",
    "        node_ids_df['~id'] = node_ids_df['~id_tmp'].apply(lambda col: f'{ntype}-{col}')\n",
    "        node_ids_df['node_id'] = node_id_list\n",
    "\n",
    "        # create feature dataframe columns\n",
    "        cols = {'val' + str(i + 1): node_feats[:, i] for i in range(num_feats)}\n",
    "        node_feats_df = pd.DataFrame(cols)\n",
    "        json_props_df = node_feats_df.apply(lambda row: json.dumps(dict(row), default=str), axis=1).to_frame('props_values:String') \n",
    "\n",
    "        # merge id with feature, where feature_df use index\n",
    "        node_id_feats_df = node_ids_df.merge(json_props_df, left_on='node_id', right_on=json_props_df.index)\n",
    "        # drop the id_tmp and node_id columns to follow the Grelim format requirements\n",
    "        node_id_feats_df = node_id_feats_df.drop(['~id_tmp', 'node_id'], axis=1)\n",
    "\n",
    "        # dump the embeddings to files\n",
    "        node_id_feats_df.to_csv(os.path.join(model_dir, ntype + '.csv'),\n",
    "                                index=False, header=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "def get_model(ntype_dict, etypes, hyperparams, in_feats, n_classes, device):\n",
    "\n",
    "    model = HeteroRGCN(ntype_dict, etypes, in_feats, hyperparams['n_hidden'], n_classes, hyperparams['n_layers'], in_feats)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--training-dir', type=str, default= 'preprocessed_data')\n",
    "    parser.add_argument('--model-dir', type=str, default= 'model-dir')\n",
    "    parser.add_argument('--output-dir', type=str, default= 'output-dir')\n",
    "    parser.add_argument('--nodes', type=str, default='features.csv')\n",
    "    parser.add_argument('--target-ntype', type=str, default='TransactionID')\n",
    "    parser.add_argument('--edges', type=str, default='relation*')\n",
    "    parser.add_argument('--labels', type=str, default='tags.csv')\n",
    "    parser.add_argument('--new-accounts', type=str, default='test.csv')\n",
    "    parser.add_argument('--compute-metrics', type=lambda x: (str(x).lower() in ['true', '1', 'yes']),\n",
    "                        default=True, help='compute evaluation metrics after training')\n",
    "    parser.add_argument('--threshold', type=float, default=0, help='threshold for making predictions, default : argmax')\n",
    "    parser.add_argument('--num-gpus', type=int, default=0)\n",
    "    parser.add_argument('--optimizer', type=str, default='adam')\n",
    "    parser.add_argument('--lr', type=float, default=4e-3)\n",
    "    parser.add_argument('--n-epochs', type=int, default=10)\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, help='number of hidden units')\n",
    "    parser.add_argument('--n-layers', type=int, default=2, help='number of hidden layers')\n",
    "    parser.add_argument('--weight-decay', type=float, default=5e-4, help='Weight for L2 loss')\n",
    "    parser.add_argument('--dropout', type=float, default=0.2, help='dropout probability, for gat only features')\n",
    "    parser.add_argument('--embedding-size', type=int, default=64, help=\"embedding size for node embedding\")\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['tid','sid','mid']] = data[['tid','sid','mid']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('date',axis=1).to_csv('../fraud-challenge/input_data/transaction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(id_to_node, node_feature_files):\n",
    "    \"\"\"\n",
    "\n",
    "    :param id_to_node: dictionary mapping node names(id) to dgl node idx\n",
    "    :param node_features: path to file containing node features\n",
    "    :return: (np.ndarray, list) node feature matrix in order and new nodes not yet in the graph\n",
    "    \"\"\"\n",
    "    indices, features, new_nodes = [], [], []\n",
    "    max_node = max(id_to_node.values())\n",
    "\n",
    "    features = node_feature_files.iloc[:,1:].copy()\n",
    "    \n",
    "    for node_file in node_feature_files:\n",
    "        is_1st_line = True\n",
    "        with open(node_file, \"r\") as fh:\n",
    "            for line in fh:\n",
    "                # hard-coding to ignore the 1st line of header\n",
    "                if is_1st_line:\n",
    "                    is_1st_line = False\n",
    "                    continue\n",
    "    \n",
    "                node_feats = line.strip().split(\",\")\n",
    "                node_id = node_feats[0]\n",
    "                feats = np.array(list(map(float, node_feats[1:])))\n",
    "                features.append(feats)\n",
    "                if node_id not in id_to_node:\n",
    "                    max_node += 1\n",
    "                    id_to_node[node_id] = max_node\n",
    "                    new_nodes.append(max_node)\n",
    "    \n",
    "                indices.append(id_to_node[node_id])\n",
    "\n",
    "    features = np.array(features).astype('float32')\n",
    "    features = features[np.argsort(indices), :]\n",
    "    return features, new_nodes\n",
    "\n",
    "def loadDF(f):\n",
    "    try:\n",
    "        return pd.read_csv(f)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_labels(id_to_node, n_nodes, target_node_type, labels_files, masked_nodes_files, additional_mask_rate=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param id_to_node: dictionary mapping node names(id) to dgl node idx\n",
    "    :param n_nodes: number of user nodes in the graph\n",
    "    :param target_node_type: column name for target node type\n",
    "    :param labels_path: filepath containing labelled nodes\n",
    "    :param masked_nodes_path: filepath containing list of nodes to be masked\n",
    "    :param additional_mask_rate: additional_mask_rate: float for additional masking of nodes with labels during training\n",
    "    :return: (list, list) train and test mask array\n",
    "    \"\"\"\n",
    "    node_to_id = {v: k for k, v in id_to_node.items()}\n",
    "\n",
    "    labels_df_from_files = map(loadDF, labels_files)\n",
    "    user_to_label = pd.concat(labels_df_from_files, ignore_index=True).set_index(target_node_type)\n",
    "\n",
    "    labels = user_to_label.loc[map(int, pd.Series(node_to_id)[np.arange(n_nodes)].values)].values.flatten()\n",
    "    masked_nodes = read_masked_nodes(masked_nodes_files)\n",
    "    train_mask, test_mask = _get_mask(id_to_node, node_to_id, n_nodes, masked_nodes,\n",
    "                                      additional_mask_rate=additional_mask_rate)\n",
    "    return labels, train_mask, test_mask\n",
    "\n",
    "\n",
    "def read_masked_nodes(masked_nodes_files):\n",
    "    \"\"\"\n",
    "    Returns a list of nodes extracted from the path passed in\n",
    "\n",
    "    :param masked_nodes_path: filepath containing list of nodes to be masked i.e test users\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    masked_nodes = []\n",
    "    for f in masked_nodes_files:\n",
    "        with open(f, \"r\") as fh:\n",
    "            masked_nodes += [line.strip() for line in fh]\n",
    "    return masked_nodes\n",
    "\n",
    "\n",
    "def _get_mask(id_to_node, node_to_id, num_nodes, masked_nodes, additional_mask_rate):\n",
    "    \"\"\"\n",
    "    Returns the train and test mask arrays\n",
    "\n",
    "    :param id_to_node: dictionary mapping node names(id) to dgl node idx\n",
    "    :param node_to_id: dictionary mapping dgl node idx to node names(id)\n",
    "    :param num_nodes: number of user/account nodes in the graph\n",
    "    :param masked_nodes: list of nodes to be masked during training, nodes without labels\n",
    "    :param additional_mask_rate: float for additional masking of nodes with labels during training\n",
    "    :return: (list, list) train and test mask array\n",
    "    \"\"\"\n",
    "    train_mask = np.ones(num_nodes)\n",
    "    test_mask = np.zeros(num_nodes)\n",
    "    for node_id in masked_nodes:\n",
    "        train_mask[id_to_node[node_id]] = 0\n",
    "        test_mask[id_to_node[node_id]] = 1\n",
    "    if additional_mask_rate and additional_mask_rate < 1:\n",
    "        unmasked = np.array([idx for idx in range(num_nodes) if node_to_id[idx] not in masked_nodes])\n",
    "        yet_unmasked = np.random.permutation(unmasked)[:int(additional_mask_rate*num_nodes)]\n",
    "        train_mask[yet_unmasked] = 0\n",
    "    return train_mask, test_mask\n",
    "\n",
    "\n",
    "def _get_node_idx(id_to_node, node_type, node_id, ptr):\n",
    "    if node_type in id_to_node:\n",
    "        if node_id in id_to_node[node_type]:\n",
    "            node_idx = id_to_node[node_type][node_id]\n",
    "        else:\n",
    "            id_to_node[node_type][node_id] = ptr\n",
    "            node_idx = ptr\n",
    "            ptr += 1\n",
    "    else:\n",
    "        id_to_node[node_type] = {}\n",
    "        id_to_node[node_type][node_id] = ptr\n",
    "        node_idx = ptr\n",
    "        ptr += 1\n",
    "\n",
    "    return node_idx, id_to_node, ptr\n",
    "\n",
    "def df_to_edgelist(edges: pd.DataFrame, id_to_node, header=False, source_type='user', sink_type='user'):\n",
    "    \"\"\"\n",
    "    Parse an edgelist path file and return the edges as a list of tuple\n",
    "    :param edges: path to comma separated file containing bipartite edges with header for edgetype\n",
    "    :param id_to_node: dictionary containing mapping for node names(id) to dgl node indices\n",
    "    :param header: boolean whether or not the file has a header row\n",
    "    :param source_type: type of the source node in the edge. defaults to 'user' if no header\n",
    "    :param sink_type: type of the sink node in the edge. defaults to 'user' if no header.\n",
    "    :return: (list, dict) a list containing edges of a single relationship type as tuples and updated id_to_node dict.\n",
    "    \"\"\"\n",
    "    edge_list = []\n",
    "    rev_edge_list = []\n",
    "    source_pointer, sink_pointer = 0, 0\n",
    "    cols = edges.columns\n",
    "    \n",
    "    if header:\n",
    "        source_type, sink_type = cols[0], cols[1]\n",
    "        if source_type in id_to_node:\n",
    "            source_pointer = max(id_to_node[source_type].values()) + 1\n",
    "        if sink_type in id_to_node:\n",
    "            sink_pointer = max(id_to_node[sink_type].values()) + 1\n",
    "        \n",
    "    for idx, row in edges.iterrows():\n",
    "        source, sink = row[cols[0]], row[cols[1]]\n",
    "            \n",
    "        source_node, id_to_node, source_pointer = _get_node_idx(id_to_node, source_type, source, source_pointer)\n",
    "        if source_type == sink_type:\n",
    "            sink_node, id_to_node, source_pointer = _get_node_idx(id_to_node, sink_type, sink, source_pointer)\n",
    "        else:\n",
    "            sink_node, id_to_node, sink_pointer = _get_node_idx(id_to_node, sink_type, sink, sink_pointer)\n",
    "\n",
    "        edge_list.append((source_node, sink_node))\n",
    "        rev_edge_list.append((sink_node, source_node))\n",
    "\n",
    "    return edge_list, rev_edge_list, id_to_node, source_type, sink_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(edges, nodes, target_node_type):\n",
    "\n",
    "    print(\"Getting relation graphs from the following edge lists : {} \".format(edges))\n",
    "    edgelists, id_to_node = {}, {}\n",
    "    for i, edge in enumerate(edges):\n",
    "        edgelist, rev_edgelist, id_to_node, src, dst = df_to_edgelist(edge, id_to_node, header=True)\n",
    "        if src == target_node_type:\n",
    "            src = 'target'\n",
    "        if dst == target_node_type:\n",
    "            dst = 'target'\n",
    "\n",
    "        if src == 'target' and dst == 'target':\n",
    "            print(\"Will add self loop for target later......\")\n",
    "        else:\n",
    "            if (src, src + '<>' + dst, dst) in edgelists:\n",
    "                edgelists[(src, src + '<>' + dst, dst)] = edgelists[(src, src + '<>' + dst, dst)] + edgelist\n",
    "                edgelists[(dst, dst + '<>' + src, src)] = edgelists[(dst, dst + '<>' + src, src)] +rev_edgelist\n",
    "                print(\"Append edges for {} from edgelist: {}\".format(src + '<>' + dst, edge))\n",
    "            else:\n",
    "                edgelists[(src, src + '<>' + dst, dst)] = edgelist\n",
    "                edgelists[(dst, dst + '<>' + src, src)] = rev_edgelist\n",
    "                print(\"Read edges for {} from edgelist: {}\".format(src + '<>' + dst, edge))\n",
    "\n",
    "    # get features for target nodes\n",
    "    features, new_nodes = get_features(id_to_node[target_node_type], nodes)\n",
    "    print(\"Read in features for target nodes\")\n",
    "\n",
    "    # add self relation\n",
    "    edgelists[('target', 'self_relation', 'target')] = [(t, t) for t in id_to_node[target_node_type].values()]\n",
    "\n",
    "    g = dgl.heterograph(edgelists)\n",
    "    print(\n",
    "        \"Constructed heterograph with the following metagraph structure: Node types {}, Edge types{}\".format(\n",
    "            g.ntypes, g.canonical_etypes))\n",
    "    print(\"Number of nodes of type target : {}\".format(g.number_of_nodes('target')))\n",
    "\n",
    "    g.nodes['target'].data['features'] = th.from_numpy(features)\n",
    "\n",
    "    target_id_to_node = id_to_node[target_node_type]\n",
    "    id_to_node['target'] = target_id_to_node\n",
    "\n",
    "    del id_to_node[target_node_type]\n",
    "\n",
    "    return g, features, target_id_to_node, id_to_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relation_sid_edgelist', 'relation_mid_edgelist']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in processed_data.keys() if 'relation' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_data = [processed_data[name].astype(int) for name in processed_data.keys() if 'relation' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = get_logger(__name__)\n",
    "\n",
    "print('numpy version:{} PyTorch version:{} DGL version:{}'.format(np.__version__,\n",
    "                                                                    th.__version__,\n",
    "                                                                    dgl.__version__))\n",
    "\n",
    "edges_data = [processed_data[name] for name in processed_data.keys() if 'relation' in name] \n",
    "\n",
    "g, features, target_id_to_node, id_to_node = construct_graph( edges_data,\n",
    "                                                                ,\n",
    "                                                                LABEL_COL)\n",
    "\n",
    "mean, stdev, features = normalize(th.from_numpy(features))\n",
    "\n",
    "print('feature mean shape:{}, std shape:{}'.format(mean.shape, stdev.shape))\n",
    "\n",
    "g.nodes['target'].data['features'] = features\n",
    "\n",
    "print(\"Getting labels\")\n",
    "n_nodes = g.number_of_nodes('target')\n",
    "\n",
    "labels, _, test_mask = get_labels(target_id_to_node,\n",
    "                                            n_nodes,\n",
    "                                            args.target_ntype,\n",
    "                                            get_files(args.labels, args.training_dir),\n",
    "                                            get_files(args.new_accounts, args.training_dir))\n",
    "print(\"Got labels\")\n",
    "\n",
    "labels = th.from_numpy(labels).float()\n",
    "test_mask = th.from_numpy(test_mask).float()\n",
    "\n",
    "n_nodes = th.sum(th.tensor([g.number_of_nodes(n_type) for n_type in g.ntypes]))\n",
    "n_edges = th.sum(th.tensor([g.number_of_edges(e_type) for e_type in g.etypes]))\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "            #Nodes: {}\n",
    "            #Edges: {}\n",
    "            #Features Shape: {}\n",
    "            #Labeled Test samples: {}\"\"\".format(n_nodes,\n",
    "                                                    n_edges,\n",
    "                                                    features.shape,\n",
    "                                                    test_mask.sum()))\n",
    "\n",
    "if args.num_gpus:\n",
    "    cuda = True\n",
    "    device = th.device('cuda:0')\n",
    "else:\n",
    "    cuda = False\n",
    "    device = th.device('cpu')\n",
    "\n",
    "print(\"Initializing Model\")\n",
    "in_feats = features.shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "ntype_dict = {n_type: g.number_of_nodes(n_type) for n_type in g.ntypes}\n",
    "\n",
    "model = get_model(ntype_dict, g.etypes, vars(args), in_feats, n_classes, device)\n",
    "print(\"Initialized Model\")\n",
    "\n",
    "features = features.to(device)\n",
    "\n",
    "labels = labels.long().to(device)\n",
    "test_mask = test_mask.to(device)\n",
    "\n",
    "loss = th.nn.CrossEntropyLoss(th.tensor([1, 12.]))\n",
    "\n",
    "# print(model)\n",
    "optim = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "print(\"Starting Model training\")\n",
    "model, class_preds, pred_proba = train_fg(model, optim, loss, features, labels, g, g,\n",
    "                                            test_mask, device, args.n_epochs,\n",
    "                                            args.threshold,  args.compute_metrics)\n",
    "print(\"Finished Model training\")\n",
    "\n",
    "print(\"Saving model\")\n",
    "save_model(g, model, args.model_dir, id_to_node, mean, stdev)\n",
    "print(\"Model and metadata saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46b936ff1622c326d442b837a2b71fa5dbcf20e52e3a75b5ec5a0d8bfc7f438d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
